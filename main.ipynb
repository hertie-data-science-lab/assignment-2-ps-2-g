{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problem set 2\n",
    "\n",
    "## Team\n",
    "Please write here your names and team number.\n",
    "\n",
    "* Team name: PS 2 G\n",
    "* Team members: Minho Kang (239742)\n",
    "                Padma Bareddy (236167)\n",
    "                Saurav Kumar Jha (249354)\n",
    "\n",
    "## Using Colab with GitHub\n",
    "To utilize GPU support for model training, we highly recommend to open this notebook with Google Colab. Simply, change the domain from 'github.com' to 'githubtocolab.com' and refresh the site to open the notebook in Colab.\n",
    "If you haven't used Colab before with private repositories, make sure to grant Colab access to your private repositories (see screenshot) and after that just try to change the domain again.\n",
    "\n",
    "Finally, you should make sure that you add a GPU to your Colab notebook. You can do so by clicking on `Runtime` →  `Change runtime type` → `Hardware accelerator`  →  `GPU`.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Make sure that you always commit and push the changes you make in Colab back to GitHub. To do so from within a Colab notebook, click `File` → `Save a copy in GitHub`. You will be prompted to add a commit message, and after you click OK, the notebook will be pushed to your repository. Only changes that are visible in your GitHub repository on the main branch will be considered for grading. If you close Colab in your browser without pushing your changes to GitHub or saving them on Google Drive, they will be lost.\n",
    "\n",
    "Make sure that all your work has been pushed to GitHub before the deadline.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that the GPU  enabled in your colab notebook by running the cell below."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "# Check is GPU is enabled\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "# Get specific GPU model\n",
    "if str(device) == \"cuda:0\":\n",
    "  print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You will be working with the EuroSAT dataset. The dataset contains 8489 pictures of 3 different land coverage types (crop, herbaceous vegetation and river). Running the lines below will download the data and return a random picture from the dataset."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchvision.datasets import EuroSAT\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data = EuroSAT(root=os.getcwd(), download=True) #downloads the dataset to your current directory\n",
    "print(f\"The dataset has {len(data)} images\")\n",
    "randint = np.random.randint(len(data))\n",
    "\n",
    "pic, tar = data[randint]\n",
    "print(f\"Picture number {randint} with label: {tar}\")\n",
    "pic"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: Transform the data (10 pt)\n",
    "\n",
    " Your task is to train a classifier to classify the different land usage types in the dataset. We want to select only the 50 most frequent people in the dataset, all the other people should be mapped to a common class.\n",
    "- Implement the class `rotate` that maps pictures to flipped pictures by 90, 180, 270 or 360°. The class should return an error if you try to rotate the picture by other degrees.\n",
    "- Plot a histogram with the frequencies of each class. Make sure to insert both name and label in the histogram (e.g. `AnnualCrop:0`).\n",
    "- We create a class `RotateEuroSAT` that takes as input the original dataset and returns a new dataset which contains randomly rotated pictures and whose label proportion can be customized.\n",
    "Implement the class method `_create_rotated_dataset` that returns this pictures using the previously implemented `rotate`.\n",
    "- `RotateEuroSAT` should also take care of transforming the pictures to tensors and optionally move the tensor to a GPU device."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "from torch.utils.data import Subset, Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "def rotate_picture(picture, rotation):\n",
    "  \"\"\"\n",
    "  Rotate the image by the given degrees: 90, 180, 270, or 360.\n",
    "\n",
    "  :param picture: Image to rotate\n",
    "  :param rotation: Rotation in degrees\n",
    "  :return: Rotated image\n",
    "  \"\"\"\n",
    "\n",
    "  # raise error\n",
    "  if rotation not in [90, 180, 270, 360]:\n",
    "    raise ValueError(\"Rotation angle should be 90, 180, 270, or 360\")\n",
    "\n",
    "  return picture.rotate(rotation)\n",
    "\n",
    "\n",
    "def plot_histogram(data):\n",
    "  \"\"\"\n",
    "\n",
    "  Plot a histogram with the frequencies of each class.\n",
    "\n",
    "  data: EuroSAT dataset\n",
    "  return: fig, ax\n",
    "  \"\"\"\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "  # Extract labels from dataset\n",
    "  labels = [item[1] for item in data]\n",
    "\n",
    "  # Get class names from the dataset\n",
    "  classes_names = data.classes\n",
    "\n",
    "  # calculate frequencies of each class\n",
    "  label_counts = Counter(labels)\n",
    "\n",
    "  # prepare data for plotting\n",
    "  label_indices = list(label_counts.keys())\n",
    "  counts = list(label_counts.values())\n",
    "  class_labels = [classes_names[idx] for idx in label_indices]  # Map labels to class names\n",
    "\n",
    "\n",
    "  # plot the histogram\n",
    "  ax.bar(label_indices, counts)\n",
    "  ax.set_xlabel('Class')\n",
    "  ax.set_ylabel('Frequency')\n",
    "  ax.set_title('Class Frequency Histogram')\n",
    "\n",
    "  # annotating each bar with the count\n",
    "  for i, count in enumerate(counts):\n",
    "    ax.text(label_indices[i], count + 0.1, f\"{class_labels[i]}:{label_indices[i]}\",\n",
    "            ha='center',rotation=45, fontsize=9)\n",
    "\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "new_pic = rotate_picture(pic, 90) # Example of rotating a picture by 90 degrees\n",
    "same_pic = rotate_picture(pic, 360) # Example of rotating a picture by 360 degrees (should return the same picture)\n",
    "fig, ax = plot_histogram(data)\n",
    "fig.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(pic)\n",
    "axes[0].set_title(\"Original picture\")\n",
    "axes[1].imshow(new_pic)\n",
    "axes[1].set_title(\"Rotated by 90°\")\n",
    "axes[2].imshow(same_pic)\n",
    "axes[2].set_title(\"Rotated by 360°\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RotateEuroSAT(Dataset):\n",
    "    \"\"\"\n",
    "    Initialize a rotated EuroSAT dataset.\n",
    "\n",
    "    This dataset takes an existing EuroSAT dataset and applies random rotations (90°, 180°, 270°, or 360°)\n",
    "    to the images, and splits them based on the provided class distribution (shares).\n",
    "\n",
    "    :param original_data: The original EuroSAT dataset.\n",
    "    :param length: The desired length of the new dataset (i.e., total number of images after rotation).\n",
    "    :param shares: The distribution shares for each class. The list must sum to 1, and its length must match\n",
    "                    the number of classes in the original dataset.\n",
    "    :param device: The device (CPU or GPU) to move the images to. Default is None (images will stay on the CPU).\n",
    "    :param seed: The random seed for reproducibility. Default is 42.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 original_data: Dataset,\n",
    "                 length: int,\n",
    "                 shares: list,\n",
    "                 device=None,\n",
    "                 seed=42):\n",
    "        self.original_data = original_data\n",
    "        self.length = length\n",
    "        assert sum(shares) == 1, \"Shares must sum to 1\"\n",
    "        assert len(shares) == len(original_data.classes), \"Shares must match number of classes\"\n",
    "        self.shares = shares\n",
    "        self.seed = seed\n",
    "        self.device = device\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.dataset = self._create_rotated_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the rotated dataset.\n",
    "\n",
    "        :return: The length of the dataset (i.e., the number of images in the rotated dataset).\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item (rotated image and its label) from the dataset.\n",
    "\n",
    "        :param idx: The index of the item to fetch from the dataset.\n",
    "        :type idx: int\n",
    "        :return: A tuple of the rotated image (tensor) and the label.\n",
    "        :rtype: tuple (torch.Tensor, int)\n",
    "        \"\"\"\n",
    "        picture, label = self.dataset[idx]\n",
    "\n",
    "        # Apply transformation to tensor\n",
    "        picture = self.transform(picture)\n",
    "\n",
    "        # Optionally move the tensor to the specified device (CPU/GPU)\n",
    "        if self.device is not None:\n",
    "            picture = picture.to(self.device)\n",
    "\n",
    "        return picture, label\n",
    "\n",
    "    def _create_rotated_dataset(self):\n",
    "        \"\"\"\n",
    "        Create a dataset with rotated images based on the given shares for each class.\n",
    "\n",
    "        This method rotates images from the original dataset randomly by one of the following angles:\n",
    "        90°, 180°, 270°, or 360°.\n",
    "\n",
    "        :return: A list of rotated images and their corresponding labels.\n",
    "        :rtype: list of tuple (PIL.Image, int)\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "        rotated_dataset = []\n",
    "\n",
    "        # Set a start index\n",
    "        start_idx = 0\n",
    "\n",
    "        # Create list of the number of each class\n",
    "        labels = [item[1] for item in self.original_data]\n",
    "        label_counts_lst = list(Counter(labels).values())\n",
    "\n",
    "        for class_idx, class_num in enumerate(label_counts_lst):\n",
    "            # Calculate the number of images for this class based on the share\n",
    "            class_share = self.shares[class_idx]\n",
    "            num_class_images = int(class_share * self.length)\n",
    "            class_images = [i for i in range(start_idx, start_idx + class_num) if self.original_data[i][1] == class_idx]\n",
    "\n",
    "            # Rotate images\n",
    "            for img_idx in class_images:\n",
    "                image, label = self.original_data[img_idx]\n",
    "\n",
    "                # Rotate the image by one of the allowed angles (90, 180, 270, 360)\n",
    "                rotation_angle = random.choice([90, 180, 270, 360])\n",
    "                rotated_image = image.rotate(rotation_angle)\n",
    "\n",
    "                # Add rotated image and label to the new dataset\n",
    "                rotated_dataset.append((rotated_image, label))\n",
    "\n",
    "            start_idx += class_num\n",
    "\n",
    "        return rotated_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rotated_data = RotateEuroSAT(data,\n",
    "                             length=10**4,\n",
    "                             shares=[1 / len(data.classes) for _ in data.classes],\n",
    "                             device=device,\n",
    "                             seed=42,)\n",
    "\n",
    "train_data, test_data = random_split(rotated_data, [0.8, 0.2])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: Implement a max pooling class and a CNN model(15 pt)\n",
    "Implement a classification model to predict the label of the faces in the dataset. You are free to experiment with the network architecture. However your model **must** contain:\n",
    "- At least one max pooling layer, implemented with `MyMaxPool`,\n",
    "- Convolutional, linear, and pooling layers only,\n",
    "- At least 3 convolutional layers, with at least two different kernel sizes,\n",
    "- A final output layer that is customizable to the number of classes that we want to predict.\n",
    "\n",
    "#### Briefly explain why you chose the particular architecture you implemented (around 2-3 sentences).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "class MyMaxPool(nn.Module):\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        super(MyMaxPool, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self,x):\n",
    "        return nn.functional.max_pool2d(x,self.kernel_size, self.stride)\n",
    "\n",
    "\n",
    "\n",
    "class MyCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize a CNN model with lazy layers that infer input dimensions\n",
    "        automatically.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): The number of output classes.\n",
    "        \"\"\"\n",
    "        super(MyCNNModel, self).__init__()\n",
    "\n",
    "        # First Lazy Convolutional Layer (input channels will be inferred)\n",
    "        self.conv1 = nn.LazyConv2d(out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Second Lazy Convolutional Layer (input channels will be inferred)\n",
    "        self.conv2 = nn.LazyConv2d(out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "        # Third Lazy Convolutional Layer (input channels will be inferred)\n",
    "        self.conv3 = nn.LazyConv2d(out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Max Pooling Layer\n",
    "        self.pool = MyMaxPool(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layer will be created lazily (input size inferred)\n",
    "        self.fc1 = nn.LazyLinear(out_features=64)  # Output will be inferred during the first forward pass\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=num_classes)  # The final output layer based on the number of classes\n",
    "\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through convolutional layers with ReLU activation and pooling\n",
    "        x = self.relu(self.conv1(x))  # First conv layer\n",
    "        x = self.pool(x)  # Pooling\n",
    "\n",
    "        x = self.relu(self.conv2(x))  # Second conv layer\n",
    "        x = self.pool(x)  # Pooling\n",
    "\n",
    "        x = self.relu(self.conv3(x))  # Third conv layer\n",
    "        x = self.pool(x)  # Pooling\n",
    "\n",
    "        # Flatten the output before passing to fully connected layers\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Fully connected layers with ReLU activation\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Final output layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_x, train_y = train_data[0]"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#print one iteration of your model to test its correctness\n",
    "\n",
    "my_model = MyCNNModel(num_classes=10).to(device)  # 10 classes for EUROSET database\n",
    "output = my_model(train_x)\n",
    "print(\" Model test for correctness\")\n",
    "print(\"Input shape:\", train_x.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "We define a `Trainer` function to train our model that returns avg loss and avg accuracy per epoch. We set the configuration of the trainer is set in the `cfg` dictionary. Use the trainer to train your model and make sure to print and plot avg loss and accuracy using the in-built commands."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, datetime, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "cfg = {\n",
    "    'batch_size': 64,\n",
    "    'criterion': 'CrossEntropyLoss', #change to 'nn.NLLLoss' if you are applying a softmax in the last layer of your model\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'optimizer':'Adam',\n",
    "    'seed':42,\n",
    "\n",
    "}\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for training, evaluating, and logging the model's performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Sequential,\n",
    "                 cfg:dict,\n",
    "                 device = None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize.\n",
    "\n",
    "        :param model: The model to be trained.\n",
    "        :param cfg: A dictionary containing hyperparameters such as batch size, optimizer, etc.\n",
    "        :param device: The device (CPU or GPU) on which the model will be trained. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.cfg = cfg\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)  # Move the model to the specified device\n",
    "\n",
    "        # lists to store loss and accuracy for plotting\n",
    "        self.all_avg_acc=[]\n",
    "        self.all_avg_loss=[]\n",
    "\n",
    "    def torch_train(self, data, shuffle=False):\n",
    "        \"\"\"\n",
    "        Train model\n",
    "\n",
    "        :param data: train data\n",
    "        :param shuffle: boolean whether shuffle or not.\n",
    "        :return: trained model\n",
    "        \"\"\"\n",
    "    # Get the loss function (criterion) based on the provided config\n",
    "        criterion = getattr(nn, self.cfg['criterion'])()  # CrossEntropyLoss, etc.\n",
    "\n",
    "        # Initialize the optimizer based on the provided config\n",
    "        optimizer = getattr(optim, self.cfg['optimizer'])(\n",
    "            self.model.parameters(), lr=self.cfg['learning_rate']\n",
    "        )\n",
    "\n",
    "        # Set the model to training mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Create DataLoader for the training data\n",
    "        dataloader = DataLoader(data,\n",
    "                                batch_size=self.cfg['batch_size'],\n",
    "                                shuffle=shuffle)\n",
    "\n",
    "        running_loss = 0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        for epoch in range(self.cfg['epochs']):\n",
    "\n",
    "          print(f\"Epoch {epoch+1}/{self.cfg['epochs']}\")\n",
    "\n",
    "          for x, y in dataloader:  # tqdm is used to display a progress bar\n",
    "\n",
    "            x = x.to(self.device)  # move input data to the device (GPU/CPU)\n",
    "            y = y.to(self.device)  # move target labels to the device (GPU/CPU)\n",
    "\n",
    "\n",
    "            # zero the gradients before the forward pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: get model predictions\n",
    "            pred = self.model(x)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(pred.squeeze(-1),y.long())\n",
    "\n",
    "            # backward pass: calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update model parameters based on gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running loss\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "\n",
    "            # calculate accuracy: compare predicted and true labels\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            correct_preds += (predicted == y).sum().item()\n",
    "            total_preds += y.size(0)\n",
    "\n",
    "          # calculate average loss and accuracy for this epoch\n",
    "          avg_loss = running_loss / total_preds\n",
    "          avg_acc = correct_preds / total_preds\n",
    "          self.all_avg_acc.append(avg_acc)\n",
    "          self.all_avg_loss.append(avg_loss)\n",
    "\n",
    "        return self.model\n",
    "    def plot_training_metric(self):\n",
    "        \"\"\"\n",
    "        Plot the training loss and accuracy per epoch.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.all_avg_loss, label='Train Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.all_avg_acc, label='Train Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training Accuracy')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def evalute_test(self, model, test_data):\n",
    "        \"\"\"\n",
    "        evalute model with test data\n",
    "\n",
    "        :param model: trained model\n",
    "        :param test_data: test data\n",
    "        :return: avg accuracy\n",
    "        \"\"\"\n",
    "        # create DataLoader for the test data\n",
    "        dataloader = DataLoader(test_data,\n",
    "                                batch_size=self.cfg['batch_size'],\n",
    "                                shuffle=False)\n",
    "        # get a batch of test data\n",
    "        x_test, y_test = next(iter(dataloader))\n",
    "        x_test = x_test.to(self.device)\n",
    "        y_test = y_test.to(self.device)\n",
    "\n",
    "        # get model predictions\n",
    "        pred = model(x_test)\n",
    "\n",
    "        # get the predicted class with the highest probability\n",
    "        y_pred = pred.argmax(dim=-1)\n",
    "\n",
    "        # calculate the average accuracy\n",
    "        avg_acc = (y_pred == y_test).float().mean().item()\n",
    "\n",
    "        return avg_acc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Assuming `train_data` and `val_data` are already defined and are instances of DataLoader\n",
    "my_model = MyCNNModel(num_classes=10).to(device)\n",
    "trainer = Trainer(model=my_model, cfg=cfg, device=device)\n",
    "train_model = trainer.torch_train(train_data)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.plot_training_metric()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(f\"Test Accurcay: {trainer.evalute_test(train_model, test_data)}\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 3: Tune your training hyperparameters (optional, 10 pt)\n",
    "\n",
    "Implement a method <code>grid_search</code>, which looks for the best possible learning rates and training batch sizes for your model <code>MyCNNModel</code> and returns the best possible model, the corresponding training configuration, and the final training avg losses and accuracies (as numbers)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def grid_search(train_dataset, cfg, learning_rates=[10**-1, 10**-2, 10**-3], batch_sizes=[2**5, 2**6, 2**7]):\n",
    "    '''#TODO: here your code '''\n",
    "    return best_model, best_cfg, best_avg_loss, best_avg_acc\n",
    "\n",
    "\n",
    "best_model, best_cfg, best_avg_loss, best_avg_acc = grid_search(train_data, cfg, learning_rates=[10**-1, 10**-2, 10**-3], batch_sizes=[2**5, 2**6, 2**7])\n",
    "print(f\"Best model achieves {best_avg_loss:.2f} loss and {best_avg_acc:.1%} accuracy\").\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 3: Load and fine-tune a pre-trained model (10 pt)\n",
    "\n",
    "<ul>\n",
    "  <li>Load and train a pre-trained model for classification problems, such as those made available in <a href=\"https://huggingface.co/docs/timm\">Hugging Face's timm library</a>. </li>\n",
    "  <li> Make sure to modify the output layer to be compatible with the number of classes. </li>\n",
    "  <li>Print a summary of your results.</li>\n",
    "  <li>Briefly explain why you chose the particular architecture you did (around 2-3 sentences).</li>\n",
    "  </ul>\n",
    "  \n",
    "<b>Note</b>: in case you run into computing-related (e.g. memory) issues, consider choosing another model."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''#TODO: import and fine-tune a pretrained model'''\n",
    "loaded_model = None #here your loaded model\n",
    "loaded_trainer = Trainer(loaded_model, cfg)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''#TODO: train your model, plot accuracy and loss by iteration (one iteration=one batch)'''\n",
    "train_loss, train_acc = loaded_trainer.train(train_data)\n",
    "fig, (ax0, ax1) = plt.subplots(1,2)\n",
    "ax0.plot(range(len(train_loss)), train_loss)\n",
    "ax1.plot(range(len(train_acc)), train_acc)\n",
    "ax0.set_title('Training loss')\n",
    "ax1.set_title('Training accuracy')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''#TODO: test your model, plot accuracy and loss by iteration (one iteration=one batch)'''\n",
    "test_loss, test_acc = loaded_trainer.test(test_data)\n",
    "fig, (ax0, ax1) = plt.subplots(1,2)\n",
    "ax0.plot(range(len(test_loss)), test_loss)\n",
    "ax1.plot(range(len(test_acc)), test_acc)\n",
    "ax0.set_title('Test loss')\n",
    "ax1.set_title('Test accuracy')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"results-and-discussion\"></a>\n",
    "# Task  4: Results and discussion (5pt)\n",
    "\n",
    "Report the final metrics and make a few comments on the overall performance for the networks you implemented (3-4 lines).\n",
    "\n",
    "| Test metric         | your model | pre-trained model | your tuned model (optional) |\n",
    "|---------------------|--------------------|-------------------|-----------------------|\n",
    "| Accuracy (train)           |              |             |                |                     \n",
    "| Loss (train)               |               |             |                |    \n",
    "| Accuracy (test)           |              |             |                |                     \n",
    "| Loss (test)               |               |             |                |              \n",
    "             \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
